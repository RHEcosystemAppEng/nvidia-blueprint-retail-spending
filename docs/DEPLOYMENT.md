# üöÄ Deployment Guide

## üìã Table of Contents

- [Overview](#-overview)
- [Prerequisites](#-prerequisites)
- [Deployment Options](#%EF%B8%8F-deployment-options)
- [Local Deployment](#-local-deployment)
- [Cloud Deployment](#%EF%B8%8F-cloud-deployment)
- [Production Deployment](#-production-deployment)
- [Configuration](#%EF%B8%8F-configuration)
- [Monitoring](#-monitoring)
- [Troubleshooting](#%EF%B8%8F-troubleshooting)

## üéØ Overview

This guide covers deploying the Retail Shopping Assistant in various environments, from local development to production. The application supports multiple deployment strategies to accommodate different hardware configurations and use cases.

## üìã Prerequisites

### System Requirements

#### Minimum Requirements
- **OS**: Ubuntu 20.04+ or equivalent Linux distribution
- **CPU**: 8+ cores
- **RAM**: 32GB system memory
- **Storage**: 50GB available disk space
- **Network**: Stable internet connection

#### Recommended Requirements
- **OS**: Ubuntu 22.04 LTS
- **CPU**: 16+ cores
- **RAM**: 128GB+ system memory
- **Storage**: 100GB+ available disk space
- **GPUs**: 4x H100 (for local NIM deployment)
- **Network**: High-speed internet connection

### Software Dependencies

#### Required Software
- **Docker**: Version 20.10+ with Docker Compose plugin
- **NVIDIA Container Toolkit**: For GPU acceleration
- **NVIDIA Drivers**: Latest compatible drivers
- **Git**: For repository cloning

#### Optional Software
- **Kubernetes**: For production orchestration
- **Helm**: For Kubernetes deployments
- **Prometheus**: For monitoring
- **Grafana**: For visualization

### NVIDIA Account Setup

1. **Create NVIDIA Account**:
   - Visit [NVIDIA NGC](https://ngc.nvidia.com/)
   - Sign up for a free account

2. **Generate API Key**:
   - Navigate to **API Keys** in your account settings
   - Generate a new API key
   - Copy the key (starts with `nvapi-`)

3. **Accept Terms**:
   - Accept the terms of service for required NIM containers
   - Ensure you have access to the NVIDIA Container Registry

## üéõÔ∏è Deployment Options

### Option 1: Local NIM Deployment (Recommended)

**Best for**: Development, testing, production with GPU resources

**Pros**:
- Maximum performance and low latency
- Complete privacy and data control
- No ongoing cloud costs
- Full customization capabilities

**Cons**:
- Requires significant GPU resources (4x H100)
- Higher initial hardware investment
- More complex setup and maintenance

### Option 2: Cloud NIM Deployment

**Best for**: Development, testing, production without local GPUs

**Pros**:
- No local GPU requirements
- Faster initial setup
- Pay-per-use pricing
- Managed infrastructure

**Cons**:
- Ongoing cloud costs
- Network latency
- Data privacy considerations
- API rate limits

### Option 3: Hybrid Deployment

**Best for**: Production with mixed requirements

**Pros**:
- Flexibility in resource allocation
- Cost optimization
- Scalability options

**Cons**:
- More complex configuration
- Network management overhead

## üè† Local Deployment

### Step 1: Environment Setup

```bash
# Clone the repository
git clone https://github.com/NVIDIA-AI-Blueprints/retail-shopping-assistant.git
cd retail-shopping-assistant

# Create NIM cache directory
export LOCAL_NIM_CACHE=~/.cache/nim
mkdir -p "$LOCAL_NIM_CACHE"
chmod a+w "$LOCAL_NIM_CACHE"

# Set environment variables
export NGC_API_KEY=your_nvapi_key_here
export LLM_API_KEY=$NGC_API_KEY
export EMBED_API_KEY=$NGC_API_KEY
export RAIL_API_KEY=$NGC_API_KEY
```

### Step 2: Verify GPU Setup

```bash
# Check NVIDIA drivers
nvidia-smi

# Verify Docker GPU support
docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi

# Check GPU memory
nvidia-smi --query-gpu=memory.total,memory.used,memory.free --format=csv
```

### Step 3: Authenticate with NVIDIA Registry

```bash
# Login to NVIDIA Container Registry
docker login nvcr.io

# Username: oauthtoken
# Password: your_nvapi_key_here
```

### Step 4: Deploy NIMs

```bash
# Start local NIMs
docker compose -f docker-compose-nim-local.yaml up -d

# Monitor NIM startup
docker compose -f docker-compose-nim-local.yaml logs -f

# Wait for all NIMs to be ready (check logs for "ready" messages)
```

### Step 5: Deploy Application

```bash
# Build and start application services
docker compose -f docker-compose.yaml up -d --build

# Monitor application startup
docker compose -f docker-compose.yaml logs -f
```

### Step 6: Verify Deployment

```bash
# Check service status
docker compose -f docker-compose.yaml ps

# Test API endpoints
curl http://localhost:8000/health
curl http://localhost:3000

# Check NIM status
docker compose -f docker-compose-nim-local.yaml ps
```

## ‚òÅÔ∏è Cloud Deployment

### Step 1: Environment Setup

```bash
# Clone the repository
git clone https://github.com/NVIDIA-AI-Blueprints/retail-shopping-assistant.git
cd retail-shopping-assistant

# Authenticate with NVIDIA Container Registry
docker login nvcr.io
# Use oauthtoken as the username and your NGC API key as the password

# Set environment variables for cloud NIMs
export NGC_API_KEY=your_nvapi_key_here
export LLM_API_KEY=$NGC_API_KEY
export EMBED_API_KEY=$NGC_API_KEY
export RAIL_API_KEY=$NGC_API_KEY
```

### Step 2: Configure Cloud Endpoints

# Set environment variable
export CONFIG_OVERRIDE=config-build.yaml

### Step 3: Deploy Application

```bash
# Start application services only
docker compose -f docker-compose.yaml up -d --build

# Monitor startup
docker compose -f docker-compose.yaml logs -f
```

### Step 4: Verify Deployment

```bash
# Check service status
docker compose -f docker-compose.yaml ps
```

## üè≠ Production Deployment

### Kubernetes Deployment

#### Prerequisites
- Kubernetes cluster (1.24+)
- Helm (3.0+)
- NVIDIA GPU Operator installed
- Ingress controller configured

#### Step 1: Create Namespace

```bash
kubectl create namespace retail-assistant
kubectl config set-context --current --namespace=retail-assistant
```

#### Step 2: Create ConfigMap

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: retail-assistant-config
data:
  config.yaml: |
    llm_port: "https://api.nvcf.nvidia.com/v1/chat/completions"
    llm_name: "meta/llama-3.1-70b-instruct"
    retriever_port: "https://api.nvcf.nvidia.com/v1/embeddings"
    memory_port: "http://memory-retriever:8011"
    rails_port: "https://api.nvcf.nvidia.com/v1/chat/completions"
    memory_length: 16384
    top_k_retrieve: 4
    multimodal: true
```

#### Step 3: Create Secret

```bash
kubectl create secret generic nvidia-api-keys \
  --from-literal=ngc-api-key=your_nvapi_key_here \
  --from-literal=llm-api-key=your_nvapi_key_here \
  --from-literal=embed-api-key=your_nvapi_key_here \
  --from-literal=rail-api-key=your_nvapi_key_here
```

#### Step 4: Deploy with Helm

```bash
# Add Helm repository (if using a chart)
helm repo add retail-assistant https://charts.example.com
helm repo update

# Deploy the application
helm install retail-assistant retail-assistant/retail-assistant \
  --namespace retail-assistant \
  --set nvidiaApiKey=your_nvapi_key_here
```

### Docker Swarm Deployment

#### Step 1: Initialize Swarm

```bash
docker swarm init
```

#### Step 2: Create Secrets

```bash
echo "your_nvapi_key_here" | docker secret create ngc-api-key -
echo "your_nvapi_key_here" | docker secret create llm-api-key -
echo "your_nvapi_key_here" | docker secret create embed-api-key -
echo "your_nvapi_key_here" | docker secret create rail-api-key -
```

#### Step 3: Deploy Stack

```bash
docker stack deploy -c docker-compose.prod.yaml retail-assistant
```

## ‚öôÔ∏è Configuration

### Environment Variables

| Variable | Description | Required | Default |
|----------|-------------|----------|---------|
| `NGC_API_KEY` | NVIDIA NGC API key | Yes | - |
| `LLM_API_KEY` | Language model API key | Yes | - |
| `EMBED_API_KEY` | Embedding model API key | Yes | - |
| `RAIL_API_KEY` | Guardrails API key | Yes | - |
| `LOCAL_NIM_CACHE` | NIM cache directory | Local only | `~/.cache/nim` |
| `LOG_LEVEL` | Logging level | No | `INFO` |
| `NODE_ENV` | Node environment | No | `production` |

### Configuration File

The main configuration is in `chain_server/config/config.yaml`:

```yaml
# NIM Endpoints
llm_port: "http://localhost:8000/v1"  # or cloud endpoint
llm_name: "meta/llama-3.1-70b-instruct"
retriever_port: "http://localhost:8010"
memory_port: "http://localhost:8011"
rails_port: "http://localhost:8012"

# Agent Prompts
routing_prompt: |
  You are a retail store assistant that routes customer queries...

chatter_prompt: |
  You are a helpful shopping assistant specializing in...
```

### Updating Categories

The system uses a static list of product categories for classification and retrieval. These categories are defined in the configuration file and should be updated when new product types are added to the system.

#### Current Categories

The following categories are currently supported:
- **Bags**: Handbags, purses, clutches
- **Sunglasses**: Eyewear and sun protection
- **Dresses**: Various dress styles and lengths
- **Skirts**: Different skirt types and lengths
- **Top/Blouse/Sweater**: Upper body garments
- **Shoes**: Footwear including heels, flats, and sandals
- **Earrings**: Jewelry worn on the lobe or edge of the ear
- **Bracelets**: Jewelry worn on the wrist or arm
- **Necklaces**: Jawelry wrong around the neck

#### How to Update Categories

1. **Edit Configuration Files**: Update the categories list in `shared/configs/chain_server/config.yaml`
2. **Restart Services**: After updating categories, restart the chain server and catalog retriever services
3. **Update Product Data**: Ensure new products in your catalog are tagged with the appropriate categories
4. **Test Classification**: Verify that the LLM can properly classify queries into the new categories

#### Configuration File Location

```yaml
# shared/configs/chain_server/config.yaml
categories: [
    "bag",
    "sunglasses", 
    "dress",
    "skirt",
    "top blouse sweater",
    "shoes",
    "earrings",
    "bracelet",
    "necklace"
]
```

### Configuration Override System

The application supports a flexible configuration override system that allows you to switch between different deployment scenarios without modifying the base configuration files.

#### How It Works

1. **Base Configuration**: The application loads the base `config.yaml` file from each service's `config/` folder
2. **Override Detection**: If the `CONFIG_OVERRIDE` environment variable is set, the system looks for an override file
3. **Merge Process**: The override file values are merged into the base configuration, with override values taking precedence

#### Environment Variable

| Variable | Description | Example Values |
|----------|-------------|----------------|
| `CONFIG_OVERRIDE` | Specifies the override config file name | `config-build.yaml`, `config-custom.yaml` |

#### Default Configuration (Local NIMs)

By default, the application uses local NIM endpoints. No environment variable is needed:

```bash
# Deploy with local NIMs (default)
docker compose -f docker-compose-nim-local.yaml up -d
docker compose -f docker-compose.yaml up -d --build
```

**Chain Server Default** (`chain_server/config/config.yaml`):
```yaml
# LLM endpoint for local NIM deployment
llm_port: "http://localhost:8000/v1"
llm_name: "meta/llama-3.1-70b-instruct"
```

**Catalog Retriever Default** (`catalog_retriever/config/config.yaml`):
```yaml
# Text embedding endpoint for local NIM deployment
text_embed_port: "http://localhost:8001/v1"
text_model_name: "nvidia/nv-embedqa-e5-v5"

# Image embedding endpoint for local NIM deployment
image_embed_port: "http://localhost:8002/v1"
image_model_name: "nvidia/nvclip"
```

**Guardrails Default** (`guardrails/config/config.yml`):
```yaml
models:
  - type: content_safety
    engine: nim
    model: nvidia/llama-3.1-nemoguard-8b-content-safety
    parameters:
      base_url: http://localhost:8003/v1

  - type: topic_control
    engine: nim
    model: nvidia/llama-3.1-nemoguard-8b-topic-control
    parameters:
      base_url: http://localhost:8004/v1
```

#### Cloud NIM Deployment (`config-build.yaml`)

Use this when using NVIDIA API Catalog hosted endpoints:

```bash
# Set environment variable
export CONFIG_OVERRIDE=config-build.yaml

# Deploy without local NIMs
docker compose -f docker-compose.yaml up -d --build
```

**Chain Server Override** (`chain_server/config/config-build.yaml`):
```yaml
# LLM endpoint for build.nvidia.com
llm_port: "https://api.build.nvidia.com/v1"
llm_name: "meta/llama-3.1-70b-instruct"
```

**Catalog Retriever Override** (`catalog_retriever/config/config-build.yaml`):
```yaml
# Text embedding endpoint for build.nvidia.com
text_embed_port: "https://api.build.nvidia.com/v1"
text_model_name: "nvidia/nv-embedqa-e5-v5"

# Image embedding endpoint for build.nvidia.com
image_embed_port: "https://api.build.nvidia.com/v1"
image_model_name: "nvidia/nvclip"
```

**Guardrails Override** (`guardrails/config/config-build.yml`):
```yaml
models:
  - type: content_safety
    engine: nim
    model: nvidia/llama-3.1-nemoguard-8b-content-safety
    parameters:
      base_url: https://api.build.nvidia.com/v1

  - type: topic_control
    engine: nim
    model: nvidia/llama-3.1-nemoguard-8b-topic-control
    parameters:
      base_url: https://api.build.nvidia.com/v1
```

#### Creating Custom Override Files

You can create your own override files for custom configurations:

1. **Create the override file** in the same directory as the base config:
   ```bash
   # For chain server
   cp chain_server/config/config.yaml chain_server/config/config-custom.yaml
   
   # For catalog retriever
   cp catalog_retriever/config/config.yaml catalog_retriever/config/config-custom.yaml
   
   # For guardrails
   cp guardrails/config/config.yml guardrails/config/config-custom.yml
   ```

2. **Modify the override file** with your custom values:
   ```yaml
   # Example: Custom LLM endpoint
   llm_port: "https://your-custom-endpoint.com/v1"
   llm_name: "your-custom-model"
   
   # Example: Custom embedding endpoint
   text_embed_port: "https://your-embedding-service.com/v1"
   text_model_name: "your-embedding-model"
   
   # Example: Custom guardrails endpoints
   models:
     - type: content_safety
       engine: nim
       model: nvidia/llama-3.1-nemoguard-8b-content-safety
       parameters:
         base_url: https://your-custom-endpoint.com/v1
   ```

3. **Use the custom override**:
   ```bash
   export CONFIG_OVERRIDE=config-custom.yaml
   docker compose -f docker-compose.yaml up -d --build
   ```

#### Switching Between Configurations

To switch between different configurations:

```bash
# Use local NIMs (default - no environment variable needed)
unset CONFIG_OVERRIDE
docker compose -f docker-compose.yaml restart

# Switch to cloud NIMs
export CONFIG_OVERRIDE=config-build.yaml
docker compose -f docker-compose.yaml restart

# Use custom configuration
export CONFIG_OVERRIDE=config-custom.yaml
docker compose -f docker-compose.yaml restart
```

#### Docker Compose Integration

You can also set the override in your docker-compose files:

```yaml
# In docker-compose.yaml
services:
  chain-server:
    environment:
      - CONFIG_OVERRIDE=${CONFIG_OVERRIDE:-config-local.yaml}
  
  catalog-retriever:
    environment:
      - CONFIG_OVERRIDE=${CONFIG_OVERRIDE:-config-local.yaml}
  
  rails:
    environment:
      - CONFIG_OVERRIDE=${CONFIG_OVERRIDE:-config-local.yml}
```

Then use it:
```bash
# Use local config
CONFIG_OVERRIDE=config-local.yaml docker compose up -d

# Use cloud config
CONFIG_OVERRIDE=config-build.yaml docker compose up -d
```

### Performance Tuning

#### GPU Memory Optimization

```yaml
# In docker-compose-nim-local.yaml
environment:
  - NIM_KVCACHE_PERCENT=.5  # Adjust based on GPU memory
  - NIM_MAX_BATCH_SIZE=1    # Reduce for memory constraints
```

#### System Resource Limits

```yaml
# In docker-compose.yaml
deploy:
  resources:
    limits:
      memory: 8G
      cpus: '4.0'
    reservations:
      memory: 4G
      cpus: '2.0'
```

## üìä Monitoring

### Health Checks

```bash
# Check service health
curl http://localhost:8000/health

# Check individual services
curl http://localhost:8010/health  # Catalog retriever
curl http://localhost:8011/health  # Memory retriever
curl http://localhost:8012/health  # Guardrails
```

### Logging

```bash
# View application logs
docker compose -f docker-compose.yaml logs -f

# View NIM logs
docker compose -f docker-compose-nim-local.yaml logs -f

# View specific service logs
docker compose -f docker-compose.yaml logs -f chain-server
```

### Metrics Collection

#### Prometheus Configuration

```yaml
# prometheus.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'retail-assistant'
    static_configs:
      - targets: ['localhost:8000', 'localhost:8010', 'localhost:8011']
```

#### Grafana Dashboard

Create a Grafana dashboard with the following metrics:
- Request rate and latency
- GPU utilization
- Memory usage
- Error rates
- Response times by agent

### Alerting

Set up alerts for:
- Service health status
- High error rates
- GPU memory usage
- Response time degradation
- API key expiration

## üõ†Ô∏è Troubleshooting

### Common Issues

#### 1. NIM Container Pull Failures

**Symptoms**: Docker pull errors for nvcr.io containers

**Solutions**:
```bash
# Verify NGC API key
echo $NGC_API_KEY

# Re-authenticate
docker login nvcr.io

# Clear Docker cache
docker system prune -a

# Check network connectivity
curl -I https://nvcr.io
```

#### 2. GPU Memory Issues

**Symptoms**: CUDA out of memory errors

**Solutions**:
```bash
# Check GPU memory usage
nvidia-smi

# Reduce batch sizes in config
# Edit docker-compose-nim-local.yaml
environment:
  - NIM_KVCACHE_PERCENT=.3
  - NIM_MAX_BATCH_SIZE=1

# Restart NIMs
docker compose -f docker-compose-nim-local.yaml restart
```

#### 3. Service Startup Failures

**Symptoms**: Services fail to start or crash

**Solutions**:
```bash
# Check service logs
docker compose -f docker-compose.yaml logs

# Check resource usage
docker stats

# Verify dependencies
docker compose -f docker-compose.yaml ps

# Check port conflicts
sudo netstat -tulpn | grep :8000
```

#### 4. Performance Issues

**Symptoms**: Slow response times

**Solutions**:
```bash
# Check GPU utilization
nvidia-smi -l 1

# Monitor system resources
htop

# Check network latency (for cloud deployment)
ping api.nvcf.nvidia.com

# Optimize configuration
# Edit chain_server/app/config.yaml
top_k_retrieve: 2  # Reduce for faster responses
```

#### 5. Authentication Issues

**Symptoms**: API key errors

**Solutions**:
```bash
# Verify API key format
echo $NGC_API_KEY | head -c 10

# Check key permissions
# Ensure key has access to required NIMs

# Test API key
curl -H "Authorization: Bearer $NGC_API_KEY" \
  https://api.nvcf.nvidia.com/v1/models
```

### Debug Mode

Enable debug logging:

```bash
# Set debug environment
export LOG_LEVEL=DEBUG

# Restart services
docker compose -f docker-compose.yaml restart

# View debug logs
docker compose -f docker-compose.yaml logs -f
```

### Recovery Procedures

#### Service Recovery

```bash
# Restart specific service
docker compose -f docker-compose.yaml restart chain-server

# Restart all services
docker compose -f docker-compose.yaml restart

# Rebuild and restart
docker compose -f docker-compose.yaml up -d --build
```

#### Data Recovery

```bash
# Backup volumes
docker run --rm -v retail-shopping-assistant_milvus_data:/data \
  -v $(pwd):/backup alpine tar czf /backup/milvus_backup.tar.gz -C /data .

# Restore volumes
docker run --rm -v retail-shopping-assistant_milvus_data:/data \
  -v $(pwd):/backup alpine tar xzf /backup/milvus_backup.tar.gz -C /data
```

## üîí Security Considerations

### Network Security

- Use HTTPS in production
- Implement API authentication
- Configure firewall rules
- Use VPN for remote access

### Data Security

- Encrypt sensitive data at rest
- Use secure API keys
- Implement access controls
- Regular security updates

### Container Security

- Scan images for vulnerabilities
- Use non-root users
- Implement resource limits
- Regular image updates

## üìà Scaling

### Horizontal Scaling

```yaml
# In docker-compose.yaml
deploy:
  replicas: 3
  resources:
    limits:
      memory: 4G
      cpus: '2.0'
```

### Load Balancing

```yaml
# nginx.conf
upstream retail_assistant {
    server chain-server:8000;
    server chain-server:8001;
    server chain-server:8002;
}
```

### Auto-scaling

```yaml
# Kubernetes HPA
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: retail-assistant-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: retail-assistant
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
```

---

For more information, see the [main README](../README.md) or [API documentation](API.md). 
